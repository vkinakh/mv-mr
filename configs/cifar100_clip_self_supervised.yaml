batch_size: 256                 # batch size for training
epochs: 1000                    # number of epochs to train for
warmup_epochs: 10               # number of warmup epochs (without learning rate decay)
log_every: 100                  # frequency of logging (steps)
eval_every: 1                   # frequency of evaluating on val set (epochs)
n_workers: 16                   # number of workers for dataloader
fp16: True                      # whether to use fp16 precision
accumulate_grad_batches: 1      # number of accumulation steps
normalize_z: False              # whether to normalize z in encoder

fine_tune_from:                 # path to pre-trained model to fine-tune from

std_margin: 1                   # margin for the std values in the loss

optimizer: adam                 # type of optimizer to use. Options: adam, adamw
wd: 1e-6                        # weight decay
lr: 1e-4                        # learning rate

dataset:                        # dataset parameters
  name: cifar100                # dataset name
  size: 32                      # image size
  n_classes: 100                # number of classes
  path:
  aug_policy: cifar             # augmentation policy. Choices: autoaugment, randaugment, custom, cifar20, cifar

encoder_type: resnet            # choices: resnet
encoder:                        # encoder parameters
  out_dim: 8192-8192-8192       # number of neurons in the projection layer
  small_kernel: True            # whether to use small kernels. Small kernels are used for CIFAR100 dataset

clip:
  model_name: ViT-B-16          # clip model name
  pretrained: laion400m_e32     # clip model checkpoint

comment: cifar100_clip_self_supervised_scale_z_VIT-B-16