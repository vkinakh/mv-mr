batch_size: 512               # batch size for training
epochs: 1000                  # number of epochs to train for
warmup_epochs: 10             # number of warmup epochs (without learning rate decay)
log_every: 100                # frequency of logging (steps)
eval_every: 1                 # frequency of evaluating on val set (epochs)
n_workers: 16                 # number of workers for dataloader
fp16: True                    # whether to use fp16 precision
accumulate_grad_batches: 1    # number of accumulation steps
normalize_z: False            # whether to normalize z in encoder

fine_tune_from:               # path to pre-trained model to fine-tune from

std_margin: 1                 # margin for the std values in the loss

optimizer: adam               # type of optimizer to use. Options: adam, adamw
wd: 1e-6                      # weight decay
lr: 2e-4                      # learning rate
scheduler: warmup_cosine      # type of scheduler to use. Options: cosine, multistep, warmup_cosine

dataset:                      # dataset parameters
  name: cifar20               # dataset name
  size: 32                    # image size
  n_classes: 20               # number of classes
  path:
  aug_policy: cifar20         # augmentation policy. Choices: autoaugment, randaugment, custom, cifar20, cifar

scatnet:                      # scatnet parameters
  J: 2                        # number of scales
  shape:                      # shape of the input image
    - 32                      # height
    - 32                      # width
  L: 8                        # number of rotations (filters per scale)

encoder_type: resnet          # choices: resnet
encoder:                      # encoder parameters
  out_dim: 8192-8192-8192     # number of neurons in the projection layer
  small_kernel: True          # whether to use small kernels. Small kernels are used for CIFAR20 dataset

hog:                          # histogram of oriented gradients parameters
  nbins: 24                   # number of bins
  pool: 8                     # pooling size

comment: cifar20_self_supervised_scale_z
