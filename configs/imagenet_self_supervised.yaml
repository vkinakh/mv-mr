batch_size: 256                # batch size for training
epochs: 100                    # number of epochs to train for
warmup_epochs: 10              # number of warmup epochs (without learning rate decay)
log_every: 100                 # frequency of logging (steps)
eval_every: 1                  # frequency of evaluating on val set (epochs)
n_workers: 16                  # number of workers for dataloader
fp16: True                     # whether to use fp16 precision
accumulate_grad_batches: 8     # number of accumulation steps
normalize_z: False             # whether to normalize z in encoder

fine_tune_from:                # path to pre-trained model to fine-tune from

std_margin: 1                  # margin for the std values in the loss

wd: 1e-6                       # weight decay
lr: 1e-4                       # learning rate

dataset:                       # dataset parameters
  name: imagenet               # dataset name
  n_classes: 1000              # number of classes
  size: 224                    # image size
  path: <>                     # path to the ImageNet folder. It should contain folders 'train' and 'val'
  aug_policy: custom           # augmentation policy. Choices: autoaugment, randaugment, custom

scatnet:                       # scatnet parameters
  J: 2                         # number of scales
  shape:                       # shape of the input image
    - 224                      # height
    - 224                      # width
  L: 8                         # number of rotations (filters per scale)

encoder:                       # encoder parameters
  out_dim: 8192-8192-8192      # number of neurons in the projection layer

hog:                           # histogram of oriented gradients parameters
  nbins: 24                    # number of bins
  pool: 8                      # pooling size

comment: imagenet_self_supervised_scale_z_8192-8192-8192