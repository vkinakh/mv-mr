batch_size: 32               # batch size for training
epochs: 1000                 # number of epochs to train for
eval_every: 1                # frequency of evaluating on val set (epochs)
n_workers: 16                # number of workers for dataloader
fp16: True                   # whether to use fp16 precision
accumulate_grad_batches: 1   # number of accumulation steps

num_classes: 20              # number of classes in VOC07 dataset. Don't change this

fine_tune_from:              # path to pre-trained model to fine-tune from

wd: 1e-6                     # weight decay
lr: 1e-4                     # learning rate

comment: imagenet_voc