batch_size: 128             # batch size for training
epochs: 200                 # number of epochs to train for
eval_every: 1               # frequency of evaluating on val set (epochs)
n_workers: 16               # number of workers for dataloader
fp16: True                  # whether to use fp16 precision
accumulate_grad_batches: 1  # number of accumulation steps

fine_tune_from:             # path to pre-trained model to fine-tune from

wd: 1e-6                    # weight decay
lr: 1e-4                    # learning rate

dataset:                    # dataset to use
  name: stl10               # dataset name
  size: 96                  # image size
  n_classes: 10             # number of classes
  percentage: 1             # percentage of dataset to use for training
  path:                     # path to the dataset. Leave empty for STL10

encoder:                      # encoder parameters
  out_dim: 8192-8192-8192     # number of neurons in the projection layer
  small_kernel: True          # whether to use small kernels. Small kernels are used for STL10 dataset

comment: stl10_semi_supervised